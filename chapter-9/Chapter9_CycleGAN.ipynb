{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "t9wyQVXTmwiz"
      },
      "source": [
        "# Chapter 9: CycleGAN\n",
        "This Colab / Ipython notebook will walk you through: \n",
        "1. Install extra dependencies that Colab does not natively support\n",
        "2. Download the apple2orange dataset using the `%%bash` magic.\n",
        "3. Create the `DataLoader()` class that server as our core data structure\n",
        "4. Do all the necessary imports\n",
        "5. Define the CycleGAN in the following steps:\n",
        "\n",
        "> 5.1 Define the key hyper-parameters and flow\n",
        "\n",
        "> 5.2 Define the static methods we are going to be reusing in the generator and discriminator\n",
        "\n",
        "> 5.3 Build the generator and the discriminator\n",
        "\n",
        "> 5.4 Define the helper function for sampling our translated images\n",
        "\n",
        "> 5.5 Define the explicit training loop\n",
        "\n",
        "6. Fit the CycleGAN and talk about the results!\n",
        "\n",
        "\n",
        "--- \n",
        "\n",
        "## Helper functions \n",
        "1. Pip installing directly from GitHub the `keras-contrib` folder, which we will need for the `InstanceNormalization`. We tested this git-hash version of the repository: \n",
        "`46fcdb9384b3bc9399c651b2b43640aa54098e64`\n",
        "2. Downloading the dataset to our cloud-based—ephemeral—storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "aS7MLTe97Tz7"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# !pip install git+https://www.github.com/keras-team/keras-contrib.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "yKbFxXyASvkb"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# %%bash\n",
        "\n",
        "# FILE=apple2orange\n",
        "\n",
        "# URL=https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/$FILE.zip\n",
        "# ZIP_FILE=./datasets/$FILE.zip\n",
        "# TARGET_DIR=./datasets/$FILE/\n",
        "# wget -N $URL -O $ZIP_FILE\n",
        "# mkdir $TARGET_DIR\n",
        "# unzip $ZIP_FILE -d ./datasets/\n",
        "# rm $ZIP_FILE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LX79_4iEpIeu"
      },
      "source": [
        "## DataLoader\n",
        "We define our key data-holding object in the (hidden) cell below. With it, we can:\n",
        "1. `load_data` from disk based on the dataset name, which we will define in `CycleGAN`'s `__init__`\n",
        "2. `load_batch` during training—note this defined as a generator for greater efficiency\n",
        "3. A helper functions called `imread`, which we use in `load_data`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "Ic8zbMaV7ldk"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# import scipy\n",
        "import scipy.misc\n",
        "# import imageio\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "\n",
        "class DataLoader():\n",
        "    def __init__(self, dataset_name, img_res=(128, 128)):\n",
        "        self.dataset_name = dataset_name\n",
        "        self.img_res = img_res\n",
        "\n",
        "    def load_data(self, domain, batch_size=1, is_testing=False):\n",
        "        data_type = \"train%s\" % domain if not is_testing else \"test%s\" % domain\n",
        "        path = glob('./datasets/%s/%s/*' % (self.dataset_name, data_type))\n",
        "\n",
        "        batch_images = np.random.choice(path, size=batch_size)\n",
        "\n",
        "        imgs = []\n",
        "        for img_path in batch_images:\n",
        "            img = self.imread(img_path)\n",
        "            if not is_testing:\n",
        "                img = scipy.misc.imresize(img, self.img_res)\n",
        "\n",
        "                if np.random.random() > 0.5:\n",
        "                    img = np.fliplr(img)\n",
        "            else:\n",
        "                img = scipy.misc.imresize(img, self.img_res)\n",
        "            imgs.append(img)\n",
        "\n",
        "        imgs = np.array(imgs)/127.5 - 1.\n",
        "\n",
        "        return imgs\n",
        "\n",
        "    def load_batch(self, batch_size=1, is_testing=False):\n",
        "        data_type = \"train\" if not is_testing else \"val\"\n",
        "        path_A = glob('./datasets/%s/%sA/*' % (self.dataset_name, data_type))\n",
        "        path_B = glob('./datasets/%s/%sB/*' % (self.dataset_name, data_type))\n",
        "\n",
        "        self.n_batches = int(min(len(path_A), len(path_B)) / batch_size)\n",
        "        total_samples = self.n_batches * batch_size\n",
        "\n",
        "        # Sample n_batches * batch_size from each path list so that model sees all\n",
        "        # samples from both domains\n",
        "        path_A = np.random.choice(path_A, total_samples, replace=False)\n",
        "        path_B = np.random.choice(path_B, total_samples, replace=False)\n",
        "\n",
        "        for i in range(self.n_batches-1):\n",
        "            batch_A = path_A[i*batch_size:(i+1)*batch_size]\n",
        "            batch_B = path_B[i*batch_size:(i+1)*batch_size]\n",
        "            imgs_A, imgs_B = [], []\n",
        "            for img_A, img_B in zip(batch_A, batch_B):\n",
        "                img_A = self.imread(img_A)\n",
        "                img_B = self.imread(img_B)\n",
        "\n",
        "                img_A = scipy.misc.imresize(img_A, self.img_res)\n",
        "                img_B = scipy.misc.imresize(img_B, self.img_res)\n",
        "\n",
        "                if not is_testing and np.random.random() > 0.5:\n",
        "                        img_A = np.fliplr(img_A)\n",
        "                        img_B = np.fliplr(img_B)\n",
        "\n",
        "                imgs_A.append(img_A)\n",
        "                imgs_B.append(img_B)\n",
        "\n",
        "            imgs_A = np.array(imgs_A)/127.5 - 1.\n",
        "            imgs_B = np.array(imgs_B)/127.5 - 1.\n",
        "\n",
        "            yield imgs_A, imgs_B\n",
        "\n",
        "    def imread(self, path):\n",
        "        return scipy.misc.imread(path, mode='RGB').astype(np.float)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1enRl0furFT1"
      },
      "source": [
        "## Good old imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "4QyCA6Lq7aMc",
        "outputId": "15a472d0-43c4-4eac-f8a0-7b0e4fc6e937"
      },
      "outputs": [],
      "source": [
        "# from keras_contrib.layers.advanced_activations import PELU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ]
        }
      ],
      "source": [
        "from __future__ import print_function, division\n",
        "# import scipy\n",
        "from keras.datasets import mnist\n",
        "from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate\n",
        "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers import UpSampling2D, Conv2D\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import Adam\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lo-E3tlCrH38"
      },
      "source": [
        "## CycleGAN itself\n",
        "\n",
        "1. We define some parameters\n",
        "2. We build the discriminators and compile them\n",
        "3. We build the generators and compile them (these are a bit more complicated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "eDZ8ZPpc7h09"
      },
      "outputs": [],
      "source": [
        "class CycleGAN():\n",
        "    def __init__(self):\n",
        "        # Input shape\n",
        "        self.img_rows = 128\n",
        "        self.img_cols = 128\n",
        "        self.channels = 3\n",
        "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
        "\n",
        "        # Configure data loader\n",
        "        self.dataset_name = 'apple2orange'\n",
        "        # Use the DataLoader object to import a preprocessed dataset\n",
        "        self.data_loader = DataLoader(dataset_name=self.dataset_name,\n",
        "                                      img_res=(self.img_rows, self.img_cols))\n",
        "\n",
        "        # Calculate output shape of D (PatchGAN)\n",
        "        patch = int(self.img_rows / 2**4)\n",
        "        self.disc_patch = (patch, patch, 1)\n",
        "\n",
        "        # Number of filters in the first layer of G and D\n",
        "        self.gf = 32\n",
        "        self.df = 64\n",
        "\n",
        "        # Loss weights\n",
        "        self.lambda_cycle = 10.0                    # Cycle-consistency loss\n",
        "        self.lambda_id = 0.9 * self.lambda_cycle    # Identity loss\n",
        "\n",
        "        optimizer = Adam(0.0002, 0.5)\n",
        "        \n",
        "        # Build and compile the discriminators\n",
        "        self.d_A = self.build_discriminator()\n",
        "        self.d_B = self.build_discriminator()\n",
        "        self.d_A.compile(loss='mse',\n",
        "                         optimizer=optimizer,\n",
        "                         metrics=['accuracy'])\n",
        "        self.d_B.compile(loss='mse',\n",
        "                         optimizer=optimizer,\n",
        "                         metrics=['accuracy'])\n",
        "\n",
        "        #-------------------------\n",
        "        # Construct Computational\n",
        "        #   Graph of Generators\n",
        "        #-------------------------\n",
        "\n",
        "        # Build the generators\n",
        "        self.g_AB = self.build_generator()\n",
        "        self.g_BA = self.build_generator()\n",
        "\n",
        "        # Input images from both domains\n",
        "        img_A = Input(shape=self.img_shape)\n",
        "        img_B = Input(shape=self.img_shape)\n",
        "\n",
        "        # Translate images to the other domain\n",
        "        fake_B = self.g_AB(img_A)\n",
        "        fake_A = self.g_BA(img_B)\n",
        "        # Translate images back to original domain\n",
        "        reconstr_A = self.g_BA(fake_B)\n",
        "        reconstr_B = self.g_AB(fake_A)\n",
        "        # Identity mapping of images\n",
        "        img_A_id = self.g_BA(img_A)\n",
        "        img_B_id = self.g_AB(img_B)\n",
        "\n",
        "        # For the combined model we will only train the generators\n",
        "        self.d_A.trainable = False\n",
        "        self.d_B.trainable = False\n",
        "\n",
        "        # Discriminators determines validity of translated images\n",
        "        valid_A = self.d_A(fake_A)\n",
        "        valid_B = self.d_B(fake_B)\n",
        "\n",
        "        # Combined model trains generators to fool discriminators\n",
        "        self.combined = Model(inputs=[img_A, img_B],\n",
        "                              outputs=[valid_A, valid_B,\n",
        "                                       reconstr_A, reconstr_B,\n",
        "                                       img_A_id, img_B_id])\n",
        "        self.combined.compile(loss=['mse', 'mse',\n",
        "                                    'mae', 'mae',\n",
        "                                    'mae', 'mae'],\n",
        "                              loss_weights=[1, 1,\n",
        "                                            self.lambda_cycle, self.lambda_cycle,\n",
        "                                            self.lambda_id, self.lambda_id],\n",
        "                              optimizer=optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8YyhyRqWrdy5"
      },
      "source": [
        "## Static methods\n",
        "Two things to pay attention to:\n",
        "1. We are using a `CycleGAN` class that inherits from the `CycleGAN` class from the previous cell. As far as I know, this is the best way of defining a class across multiple cells for educational purposes. But I agree with Joel Grus—this is not the best way, but I still like notebooks.\n",
        "2. We are using `@staticmethod`, which allows us to shrink the code size by further ~25 lines. For more on [static methods I found this SO answer to be quite good](https://stackoverflow.com/questions/136097/what-is-the-difference-between-staticmethod-and-classmethod). We will use these later to more compactly write both the generator and the discriminator. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Dws0Wp-EU_2l"
      },
      "outputs": [],
      "source": [
        "class CycleGAN(CycleGAN):\n",
        "      @staticmethod\n",
        "      def conv2d(layer_input, filters, f_size=4, normalization=True):\n",
        "        \"\"\"Discriminator layer\"\"\"\n",
        "        d = Conv2D(filters, kernel_size=f_size,\n",
        "                   strides=2, padding='same')(layer_input)\n",
        "        d = LeakyReLU(alpha=0.2)(d)\n",
        "        if normalization:\n",
        "            d = InstanceNormalization()(d)\n",
        "        return d\n",
        "      \n",
        "      @staticmethod\n",
        "      def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0):\n",
        "            \"\"\"Layers used during upsampling\"\"\"\n",
        "            u = UpSampling2D(size=2)(layer_input)\n",
        "            u = Conv2D(filters, kernel_size=f_size, strides=1,\n",
        "                       padding='same', activation='relu')(u)\n",
        "            if dropout_rate:\n",
        "                u = Dropout(dropout_rate)(u)\n",
        "            u = InstanceNormalization()(u)\n",
        "            u = Concatenate()([u, skip_input])\n",
        "            return u\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bSnp4kqysP95"
      },
      "source": [
        "## Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "W-RptWJpR8SR"
      },
      "outputs": [],
      "source": [
        "class CycleGAN(CycleGAN):\n",
        "    def build_generator(self):\n",
        "        \"\"\"U-Net Generator\"\"\"\n",
        "        # Image input\n",
        "        d0 = Input(shape=self.img_shape)\n",
        "\n",
        "        # Downsampling\n",
        "        d1 = self.conv2d(d0, self.gf)\n",
        "        d2 = self.conv2d(d1, self.gf * 2)\n",
        "        d3 = self.conv2d(d2, self.gf * 4)\n",
        "        d4 = self.conv2d(d3, self.gf * 8)\n",
        "\n",
        "        # Upsampling\n",
        "        u1 = self.deconv2d(d4, d3, self.gf * 4)\n",
        "        u2 = self.deconv2d(u1, d2, self.gf * 2)\n",
        "        u3 = self.deconv2d(u2, d1, self.gf)\n",
        "\n",
        "        u4 = UpSampling2D(size=2)(u3)\n",
        "        output_img = Conv2D(self.channels, kernel_size=4,\n",
        "                            strides=1, padding='same', activation='tanh')(u4)\n",
        "\n",
        "        return Model(d0, output_img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hl6cWO0tsSKL"
      },
      "source": [
        "## Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "5WDZgbZISCNa"
      },
      "outputs": [],
      "source": [
        "class CycleGAN(CycleGAN):\n",
        "    def build_discriminator(self):\n",
        "      img = Input(shape=self.img_shape)\n",
        "\n",
        "      d1 = self.conv2d(img, self.df, normalization=False)\n",
        "      d2 = self.conv2d(d1, self.df * 2)\n",
        "      d3 = self.conv2d(d2, self.df * 4)\n",
        "      d4 = self.conv2d(d3, self.df * 8)\n",
        "\n",
        "      validity = Conv2D(1, kernel_size=4, strides=1, padding='same')(d4)\n",
        "\n",
        "      return Model(img, validity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GAL7AoG-sT2e"
      },
      "source": [
        "## Sampling function\n",
        "This is how we got the output in the Chapter. We need to run in special mode to get the translated images out, because now we are in \"testing\"/evaluation mode.\n",
        "\n",
        "We then plot the whole cycle:\n",
        "\n",
        "$ A -> \\hat B -> \\hat A$\n",
        "\n",
        "As well as:\n",
        "\n",
        "$ B -> \\hat A -> \\hat B$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ssU_MY9NTDdF"
      },
      "outputs": [],
      "source": [
        "class CycleGAN(CycleGAN):\n",
        "      def sample_images(self, epoch, batch_i):\n",
        "        r, c = 2, 3\n",
        "\n",
        "        imgs_A = self.data_loader.load_data(domain=\"A\", batch_size=1, is_testing=True)\n",
        "        imgs_B = self.data_loader.load_data(domain=\"B\", batch_size=1, is_testing=True)\n",
        "        \n",
        "        # Translate images to the other domain\n",
        "        fake_B = self.g_AB.predict(imgs_A)\n",
        "        fake_A = self.g_BA.predict(imgs_B)\n",
        "        # Translate back to original domain\n",
        "        reconstr_A = self.g_BA.predict(fake_B)\n",
        "        reconstr_B = self.g_AB.predict(fake_A)\n",
        "\n",
        "        gen_imgs = np.concatenate([imgs_A, fake_B, reconstr_A, imgs_B, fake_A, reconstr_B])\n",
        "\n",
        "        # Rescale images 0 - 1\n",
        "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "        titles = ['Original', 'Translated', 'Reconstructed']\n",
        "        fig, axs = plt.subplots(r, c)\n",
        "        cnt = 0\n",
        "        for i in range(r):\n",
        "            for j in range(c):\n",
        "                axs[i,j].imshow(gen_imgs[cnt])\n",
        "                axs[i, j].set_title(titles[j])\n",
        "                axs[i,j].axis('off')\n",
        "                cnt += 1\n",
        "        fig.savefig(\"images/%s/%d_%d.png\" % (self.dataset_name, epoch, batch_i))\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "otZDj-Rstu2q"
      },
      "source": [
        "## Training loop\n",
        "\n",
        "Here is where the real magic happens.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "iOrSHLyTSTYt"
      },
      "outputs": [],
      "source": [
        "class CycleGAN(CycleGAN):\n",
        "      def train(self, epochs, batch_size=1, sample_interval=50):\n",
        "        # Adversarial loss ground truths\n",
        "        valid = np.ones((batch_size,) + self.disc_patch)\n",
        "        fake = np.zeros((batch_size,) + self.disc_patch)\n",
        "\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            for batch_i, (imgs_A, imgs_B) in enumerate(self.data_loader.load_batch(batch_size)):\n",
        "\n",
        "                # ----------------------\n",
        "                #  Train Discriminators\n",
        "                # ----------------------\n",
        "\n",
        "                # Translate images to opposite domain\n",
        "                fake_B = self.g_AB.predict(imgs_A)\n",
        "                fake_A = self.g_BA.predict(imgs_B)\n",
        "\n",
        "                # Train the discriminators (original images = real / translated = Fake)\n",
        "                dA_loss_real = self.d_A.train_on_batch(imgs_A, valid)\n",
        "                dA_loss_fake = self.d_A.train_on_batch(fake_A, fake)\n",
        "                dA_loss = 0.5 * np.add(dA_loss_real, dA_loss_fake)\n",
        "\n",
        "                dB_loss_real = self.d_B.train_on_batch(imgs_B, valid)\n",
        "                dB_loss_fake = self.d_B.train_on_batch(fake_B, fake)\n",
        "                dB_loss = 0.5 * np.add(dB_loss_real, dB_loss_fake)\n",
        "\n",
        "                # Total discriminator loss\n",
        "                d_loss = 0.5 * np.add(dA_loss, dB_loss)\n",
        "\n",
        "                # ------------------\n",
        "                #  Train Generators\n",
        "                # ------------------\n",
        "\n",
        "                # Train the generators\n",
        "                g_loss = self.combined.train_on_batch([imgs_A, imgs_B],\n",
        "                                                      [valid, valid,\n",
        "                                                       imgs_A, imgs_B,\n",
        "                                                       imgs_A, imgs_B])\n",
        "                # If at save interval => plot the generated image samples\n",
        "                if batch_i % sample_interval == 0:\n",
        "                    self.sample_images(epoch, batch_i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "o1KuPTW7uj-K"
      },
      "source": [
        "## Train the CycleGAN\n",
        "\n",
        "Because we used the object-oriented design, we now need to instantiate the CycleGAN and run the `train`ing method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1975
        },
        "colab_type": "code",
        "id": "045EjDTmSUBl",
        "outputId": "4e65cf4b-6404-4525-fff0-3c41d431526f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From C:\\Users\\furyx\\miniconda3\\envs\\tf1\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From C:\\Users\\furyx\\miniconda3\\envs\\tf1\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From C:\\Users\\furyx\\miniconda3\\envs\\tf1\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From C:\\Users\\furyx\\miniconda3\\envs\\tf1\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From C:\\Users\\furyx\\miniconda3\\envs\\tf1\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2018: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\furyx\\miniconda3\\envs\\tf1\\lib\\site-packages\\ipykernel_launcher.py:72: DeprecationWarning:     `imread` is deprecated!\n",
            "    `imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "    Use ``imageio.imread`` instead.\n",
            "C:\\Users\\furyx\\miniconda3\\envs\\tf1\\lib\\site-packages\\ipykernel_launcher.py:56: DeprecationWarning:     `imresize` is deprecated!\n",
            "    `imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "    Use ``skimage.transform.resize`` instead.\n",
            "C:\\Users\\furyx\\miniconda3\\envs\\tf1\\lib\\site-packages\\ipykernel_launcher.py:57: DeprecationWarning:     `imresize` is deprecated!\n",
            "    `imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "    Use ``skimage.transform.resize`` instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From C:\\Users\\furyx\\miniconda3\\envs\\tf1\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From C:\\Users\\furyx\\miniconda3\\envs\\tf1\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From C:\\Users\\furyx\\miniconda3\\envs\\tf1\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From C:\\Users\\furyx\\miniconda3\\envs\\tf1\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From C:\\Users\\furyx\\miniconda3\\envs\\tf1\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From C:\\Users\\furyx\\miniconda3\\envs\\tf1\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\furyx\\miniconda3\\envs\\tf1\\lib\\site-packages\\keras\\engine\\training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From C:\\Users\\furyx\\miniconda3\\envs\\tf1\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From C:\\Users\\furyx\\miniconda3\\envs\\tf1\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\furyx\\miniconda3\\envs\\tf1\\lib\\site-packages\\keras\\engine\\training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n",
            "C:\\Users\\furyx\\miniconda3\\envs\\tf1\\lib\\site-packages\\keras\\engine\\training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ]
        }
      ],
      "source": [
        "cycle_gan = CycleGAN()\n",
        "cycle_gan.train(epochs=100, batch_size=64, sample_interval=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YiIjwulwu1sZ"
      },
      "source": [
        "## Now we can look at the results!\n",
        "\n",
        "You may have three observations:\n",
        "1. The progress can be slow—even on the Colab GPU or on CPU.\n",
        "2. The initial images can be quite bad and make no sense at all, but worry not—soon enough, the images will manage at least a decent reconstruction.\n",
        "3. The later reconstructions are generally mixed: some look very convincing—such as the ones below—but some are merely an orange \"repainting\" of the apples and vice versa. Even the original authors of the paper mention this problem: CycleGAN is unable to substantially change the structure of the image, but only \"restylize\" it. So beware of this limitation.\n",
        "\n",
        "![Good examples](https://i.ibb.co/n00BW1F/163-10.png)\n",
        "\n",
        "Also, this is a good introduction to the reality of being a GAN researcher / practitioner. Most flashy results still tend to be cherry picked and most papers—e.g. [BigGAN being a notable exception](https://arxiv.org/abs/1809.11096)—do not share failure cases. BigGAN—the state of the art as of today (20/11/2018)—even [publishes Google folders worth of examples](https://drive.google.com/drive/folders/1SRYj8Ou1JZ4e09LqeawDHcUvoeY78tPO). "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Chapter9_CycleGAN.ipynb",
      "provenance": [],
      "version": "0.3.2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
